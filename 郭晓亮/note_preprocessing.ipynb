{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# `Scikit-learn` Preprocessing data"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"from sklearn import preprocessing\nimport numpy as np"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Standardization 标准化\n\nTransform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n\nIf a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\n**以下均按列进行标准化**\n\n### `scale()`\nIt provides a quick and easy way to perform the standardization on a single array-like dataset."},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"X_train = np.array([[ 1., -1.,  2.],\n                    [ 2.,  0.,  0.],\n                    [ 0.,  1., -1.]])\n\nX_scaled = preprocessing.scale(X_train)\nX_scaled"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"# Scaled data has zero mean and unit variance\nX_scaled.mean(axis=0)\nX_scaled.std(axis=0)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### `StandardScaler`\nIt can save the mean and standard deviation on a training set, and reapply the same transformation on the testing set / a new set. By setting `with_mean=False` or `with_std=False`, it's possible to disable centering / scaling.\n\nIternal variables:  \n- `mean_`  \n- `scale_`  \n\nMethods available:  \n- `fit()`  \n- `transform()`\n- `fit_transform()`"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"scaler = preprocessing.StandardScaler().fit(X_train)\nscaler"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"scaler.mean_"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"scaler.scale_"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"scaler.transform(X_train)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"# Reapply it to a new set\nX_test = [[-1., 1., 0.]]\nscaler.transform(X_test)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### `MinMaxScaler`\nScalie features to lie between a given minimum and maximum value, often between 0 and 1. （默认 0-1）\n\nParameters:  \n- `feature_range=(min, max)`\n\n\n\n`sklearn.preprocessing.minmax_scaler()` can achieve the same goal.\n"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"# 同样可保存mean和std值，应用于其他数组上\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train_minmax = min_max_scaler.fit_transform(X_train)\nX_train_minmax"},{"cell_type":"markdown","execution_count":20,"metadata":{},"outputs":[],"source":"### `MaxAbsScaler`\n\nThe training data stays in range `[-1,1]` by dividing through the largest maximum value in each feature. It is meant for data that is **already centered at zero** or **sparse** data.\n\n`sklearn.preprocessing.maxabs_scaler()` can achieve the same goal."},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":"max_abs_scaler = preprocessing.MaxAbsScaler()\nX_train_maxabs = max_abs_scaler.fit_transform(X_train)\nX_train_maxabs"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### Scaling sparse data\nCentering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. So `MaxAbsScaler` is specifially designed for scaling sparse data.\n\nHowever, `scale()` and `StandardScaler` can accept `scipy.sparse` matrices as input, as long as `with_mean=False` is explicitly passed to the constructor.\n\n`RobustScaler` cannot be fitted to sparse inputs, but you can use `transform()` on sparse inputs.\n\n#### Scaling data with outliers\n\nIf the data contains many outliers, scaling with mean and variance is not likely to work very well. In these cases, `robust_scale` and `RobustScaler` can be used as drop-in replacements instead.\n\n#### Scaling vs. Whitening\nSometimes a downstream model can further make some assumption on the linear independence of the features.\n\nTo address this issue `sklearn.decomposition.PCA` with `whiten=True` can be used to further remove the linear correlation across features.\n\n#### Kernel Matrices Centering\n\n`KernelCenterer` can transform the kernel matrix so that it contains inner products in the feature space defined by $\\phi$ followed by removal of the mean in that space.\n\n\n## Non-linear transformation\n\nAvailable transformations:\n- Quantile transform\n- Power transform\n\nBoth are based on monotonic transformations of the features and thus preserve the rank of the values along each feature.\n\n### Quantile transform\n\nSee here [Quantile transform](https://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation).\n\n#### Mapping to a **Uniform distribution**\n\n`QuantileTransformer` and `quantile_transform()` provide a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1.\n\n\n\n\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"quantile_transformer = preprocessing.QuantileTransformer(random_state=0)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"It is also possible to map data to a normal distribution using `QuantileTransformer` by setting `output_distribution='normal'`"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"quantile_transformer = preprocessing.QuantileTransformer(output_distribution='normal', random_state=0)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### Power transform\n\nIt's a family of parametric transformations that aim to map data from any distribution to as close to a **Gaussian distribution**, in order to stabilize variance and minimize skewness.\n\n#### Mapping to a **Gaussian distribution**\n\n`PowerTransformer` currently provides two such power transformations, the **Yeo-Johnson** transform and the **Box-Cox** transform. See [here](https://scikit-learn.org/stable/modules/preprocessing.html#mapping-to-a-gaussian-distribution).\n\n`Box-Cox` can only be applied to strictly positive data. In both methods, the transformation is parameterized by $\\lambda$, which is determined through maximum likelihood estimation.\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# With standardize=False, it means 0-mean, unit-variance\npt = preprocessing.PowerTransformer(method='box-cox', standardize=False)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Nomalization\n\nIt's the process of scaling individual samples to have **unit norm**. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n\n### `normalize()`\nIt provides a quick and easy way to perform this operation on a single array-like dataset, either using the `l1` or `l2` norms:"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":"array([[ 0.25, -0.25,  0.5 ],\n       [ 1.  ,  0.  ,  0.  ],\n       [ 0.  ,  0.5 , -0.5 ]])"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"X = [[ 1., -1.,  2.],\n     [ 2.,  0.,  0.],\n     [ 0.,  1., -1.]]\nX_normalized = preprocessing.normalize(X, norm='l2')\nX_normalized"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### `Normalizer`\n`Normalizer` implements the same operation."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"normalizer = preprocessing.Normalizer()"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Encoding categorical features\n\n### `OrdinalEncoder`\nTo convert categorical features to such integer codes, we can use the `OrdinalEncoder`. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1).\n"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":"array([[0., 1., 1.]])"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"enc = preprocessing.OrdinalEncoder()\n\nX_cat = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\nenc.fit(X_cat)  \nenc.transform([['female', 'from US', 'uses Safari']])"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### `OneHotEncoder`\nSuch integer representation can not be used directly, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired.\n\nAnother possibility is to use a one-of-K, also known as **one-hot** or **dummy** encoding, using `OneHotEncoder`, which transforms each categorical feature with `n_categories` possible values into `n_categories` binary features, with one of them 1, and all others 0."},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"array([[1., 0., 0., 1., 0., 1.],\n       [0., 1., 1., 0., 0., 1.]])"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"enc = preprocessing.OneHotEncoder()\nX_onehot = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\nenc.fit(X_onehot)  \nenc.transform([['female', 'from US', 'uses Safari'],\n               ['male', 'from Europe', 'uses Safari']]).toarray()"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"It's possible to specify the categories explicitly using the parameter `categories`."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"genders = ['female', 'male']\nlocations = ['from Africa', 'from Asia', 'from Europe', 'from US']\nbrowsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\nenc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"If the dataset might have missing categorical features, you can specify `handle_unknown='ignore'` instead of setting the categories manually as above. And no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros (`handle_unknown='ignore'` is only supported for one-hot encoding)."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"enc = preprocessing.OneHotEncoder(handle_unknown='ignore')\nX_onehot_ignore = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\nenc.fit(X_onehot_ignore) \nenc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"It's possible to encode each column into `n_categories - 1` columns instead of `n_categories` columns by using the `drop` parameter. This parameter allows the user to specify a category for each feature to be dropped.\n\nThis is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (`LinearRegression`), since co-linearity would cause the covariance matrix to be non-invertible. When this paramenter is not None, `handle_unknown` must be set to `error`.\n\n## Discretization\n\nDiscretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values, with only nominal attributes.\n\nOne-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models.\n\n### K-bins Discretization\n`KBinsDiscritizer` discretizes features into `k` bins. It can also implements different binning strategies with the `strategy` parameter:\n- `uniform` uses constant-width bins\n- `quantile` uses the quantiles values to have equally populated bins in each feature\n- `kmeans` defines bins based on a k-means clustering procedure performed on each feature independently.\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal')"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### Feature binarization\nIt's the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution.\n\n`Binarizer` is used in this case. Parameter `threshold` can be used to change the threshold $t$, if the value is smaller than $t$, then it's replaced by 0.\n\n`StandardScaler` and `Normalizer` provide a function `binarize()` that achieves the same goal."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"X_bi = [[ 1., -1.,  2.],\n        [ 2.,  0.,  0.],\n        [ 0.,  1., -1.]]\nbinarizer = preprocessing.Binarizer().fit(X_bi)  # fit does nothing\nbinarizer.transform(X_bi)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### Generating polynomial features\n\nGiven an input $(X_1, X_2, X_3, ..., X_n)$, we can transform it to polynomial features.\n\nFor example, from $(X_1, X_2)$ to $(1, X_1, X_2, X_1^2, X_1 X_2, X_2^2)$."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"X_poly = np.arange(6).reshape(3, 2)\npoly = PolynomialFeatures(2)\npoly.fit_transform(X_poly)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"Or from $(X_1, X_2, X_3)$ to $(1, X_1, X_2, X_3, X_1 X_2, X_2 X_3, X_1 X_3, X_1 X_2 X_3)$. By setting `interaction_only=True`, the $X_i^2$ or $X_i^3$ terms are ignored."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"X_poly2 = np.arange(9).reshape(3, 3)\npoly2 = PolynomialFeatures(degree=3, interaction_only=True)\npoly2.fit_transform(X_poly2)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"### Custom transformer\n\nOften, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. You can implement a transformer from an arbitrary function with [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer)."}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}